# floodai_app_v4.py
import os, re, datetime, math
import streamlit as st
import geopandas as gpd
import pandas as pd
import numpy as np
import rasterio
from rasterio.warp import transform
from shapely.geometry import Point
import folium
from streamlit_folium import st_folium



# def estimate_return_period_from_ifd(rain_mm: float, duration_h: float, lat: float, lon: float, ifd_df: pd.DataFrame) -> float:
#     """
#     Estimate rainfall return period (in years) based on the IFD table for the nearest grid/station.
#     """
#     try:
#         intensity = rain_mm / duration_h
#         duration_min = duration_h * 60

#         idx = ((ifd_df['lat'] - lat)**2 + (ifd_df['lon'] - lon)**2).idxmin()
#         site_ifd = ifd_df.loc[ifd_df['station_id'] == ifd_df.loc[idx, 'station_id']]

#         site_ifd['Duration in min'] = site_ifd['Duration in min'].astype(float)
#         row = site_ifd.iloc[(site_ifd['Duration in min'] - duration_min).abs().argsort()[:1]]

#         aep_cols = ['63.20%', '50%', '20%', '10%', '5%', '2%', '1%']
#         return_periods = [1, 2, 5, 10, 20, 50, 100]
#         aep_values = row[aep_cols].values.flatten().astype(float)

#         if intensity < aep_values.min():
#             return 1.0
#         if intensity > aep_values.max():
#             return 100.0

#         rp = np.interp(intensity, aep_values, return_periods)
#         return float(round(rp, 1))

#     except Exception as e:
#         print(f"‚ö†Ô∏è IFD estimation error: {e}")
#         return np.nan
# -----------------------------
# PATHS (edit if your tree differs)
# -----------------------------
PROPERTY_GPKG   = "SwinburneData/Property/Properties.gpkg"
FLOODMAP_ROOT   = "SwinburneData/FloodMaps/FrankstonSouth"

SUBCATCHMENT_GPKG = "SwinburneData/Subcatchments/Subcatchments.gpkg"
subcatchments_gdf = gpd.read_file(SUBCATCHMENT_GPKG)
subcatchments_gdf = subcatchments_gdf.to_crs(epsg=4326)

# --- Clean column names once when reading the CSV ---
ifd_table = pd.read_csv("SwinburneData/IFD/ifd_table.csv")
ifd_table.columns = ifd_table.columns.str.strip().str.lower().str.replace(' ', '_')
print(ifd_table.columns.tolist())




def estimate_return_period_from_ifd(rain_mm: float, duration_h: float, lat: float, lon: float, ifd_df: pd.DataFrame) -> float:
    """
    Estimate rainfall return period (years) using IFD data.
    """
    try:
        intensity = rain_mm / duration_h
        duration_min = duration_h * 60

        # Find nearest station
        idx = ((ifd_df['lat'] - lat)**2 + (ifd_df['lon'] - lon)**2).idxmin()
        site_ifd = ifd_df.loc[ifd_df['station_id'] == ifd_df.loc[idx, 'station_id']].copy()

        site_ifd['duration_in_min'] = site_ifd['duration_in_min'].astype(float)
        row = site_ifd.iloc[(site_ifd['duration_in_min'] - duration_min).abs().argsort()[:1]]
        
        
        aep_cols = ['63.20%', '50%', '20%', '10%', '5%', '2%', '1%']

        return_periods = [1, 2, 5, 10, 20, 50, 100]
        aep_values = row[aep_cols].values.flatten().astype(float)

        rp = np.interp(intensity, aep_values, return_periods)
        return float(round(rp, 1))

    except Exception as e:
        print(f"‚ö†Ô∏è IFD estimation error: {e}")
        return np.nan

# Load raster catalog dynamically
raster_catalog = pd.read_csv("SwinburneData/FloodMaps/raster_catalog.csv")

# --- Normalise catalog after reading it ---
def normalise_catalog(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    # normalise fields
    df['return_period'] = df['return_period'].astype(str).str.lower().str.strip()
    df['metric'] = df['metric'].astype(str).str.lower().str.strip()
    df['full_path'] = df['full_path'].astype(str).str.replace("\\", "/", regex=False)

    # unify synonyms (your 100y has "dmaxmax" / "hmaxmax")
    metric_map = {
        'dmaxmax': 'dmax',
        'hmaxmax': 'hmax',
        'z0':      'z0max',
        'z0_max':  'z0max'
    }
    df['metric'] = df['metric'].replace(metric_map, regex=False)

    # make sure files exist (useful warning in Streamlit)
    df['exists'] = df['full_path'].apply(lambda p: os.path.exists(p))
    missing = df.loc[~df['exists'], 'full_path'].tolist()
    if len(missing):
        st.warning(f"‚ö†Ô∏è Missing raster files (showing up to 5): {missing[:5]}")

    return df

raster_catalog = normalise_catalog(raster_catalog)
YEARS_ORDER = sorted(raster_catalog['return_period'].unique(), key=lambda x: int(x.replace('y','')))
YEAR_VALS   = np.array([int(x.replace('y','')) for x in YEARS_ORDER], dtype=float)

def collect_metrics_for_point(lat, lon):
    metrics = {y: {"dmax": np.nan, "hmax": np.nan, "vmax": np.nan, "z0max": np.nan} for y in YEARS_ORDER}

    for y in YEARS_ORDER:
        subset = raster_catalog[raster_catalog['return_period'] == y]
        for metric in ['dmax', 'hmax', 'vmax', 'z0max']:
            row = subset[subset['metric'].str.startswith(metric)]
            if not row.empty:
                path = row.iloc[0]['full_path']
                val = sample_raster_value(path,lon, lat, prefer_band=1)
                
                metrics[y][metric] = val
    return metrics


# -----------------------------
# Streamlit setup
# -----------------------------
st.set_page_config(page_title="FloodAI ‚Äî MVP v4", layout="wide")
st.title("üåßÔ∏è FloodAI ‚Äî MVP v4 (GPKG + Rasters + Clarifications + Zones)")

# -----------------------------
# Caching loaders
# -----------------------------
@st.cache_data(show_spinner=False)
def load_properties() -> gpd.GeoDataFrame:
    gdf = gpd.read_file(PROPERTY_GPKG)
    # Expect CRS EPSG:7855 in your data; keep native for metric ops, also keep WGS84 for mapping
    if gdf.crs is None:
        gdf = gdf.set_crs(epsg=7855)
    # Make a display-friendly address
    def _safe(s): return s.fillna("").astype(str).str.strip()
    gdf["Full_Address"] = (
        _safe(gdf.get("House", pd.Series([""]*len(gdf)))) + " " +
        _safe(gdf.get("Street", pd.Series([""]*len(gdf)))) + ", " +
        _safe(gdf.get("Suburb", pd.Series([""]*len(gdf)))) + " VIC " +
        _safe(gdf.get("Postcode", pd.Series([""]*len(gdf))))
    ).str.replace(r"\s+,", ",", regex=True).str.replace(r",\s+VIC\s+$"," VIC", regex=True)
    return gdf

props_gdf = load_properties()
st.caption(f"üìç Properties loaded: {len(props_gdf):,} | CRS: {props_gdf.crs}")

# @st.cache_data(show_spinner=False)
# def props_wgs84(gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
#     return gdf.to_crs(epsg=4326)



@st.cache_data(show_spinner=False)
def props_wgs84(_gdf: gpd.GeoDataFrame) -> gpd.GeoDataFrame:
    """Convert property GeoDataFrame to WGS84 for mapping (avoid hashing error)."""
    return _gdf.to_crs(epsg=4326)
props_gdf_wgs = props_wgs84(props_gdf)

# -----------------------------
# Parsing helpers (robust + clarifications)
# -----------------------------
def parse_rain(text:str):
    m = re.search(r'(\d+(?:\.\d+)?)\s*(mm|millimet(re|er|res|ers)?)', text, re.I)
    return float(m.group(1)) if m else None

def parse_duration(text:str):
    if m := re.search(r'(\d+(?:\.\d+)?)\s*(hour|hr|h|hours)', text, re.I):
        return float(m.group(1))
    if m := re.search(r'(\d+(?:\.\d+)?)\s*(minute|min|mins|m)', text, re.I):
        return float(m.group(1))/60.0
    return None

def parse_coords(text:str):
    # coordinate pair anywhere (lon/lat or lat/lon)
    m = re.search(r'(-?\d{1,3}\.\d+)[,\s]+(-?\d{1,3}\.\d+)', text)
    if not m:
        return None
    a, b = float(m.group(1)), float(m.group(2))
    # Heuristic: latitude ~ -38; longitude ~ 145 in Melbourne.
    if abs(a) < 90 and abs(b) > 90:
        return (a, b)  # (lat, lon)
    if abs(b) < 90 and abs(a) > 90:
        return (b, a)
    # if ambiguous, assume (lat,lon)
    return (a, b)

def parse_address_hint(text:str):
    # Very light rule-based extraction of ‚Äúnear <something>‚Äù or trailing phrase
    m = re.search(r'(?:near|in|at|around)\s+([A-Za-z0-9 ,\-\/]+)', text, re.I)
    return m.group(1).strip() if m else None

def clarification_needed(rain, dur, coords, addr_hint):
    need = []
    if rain is None: need.append("rainfall (mm)")
    if dur is None:  need.append("duration")
    if coords is None and not addr_hint: need.append("location (address or coordinates)")
    return need

# -----------------------------
# Property locator (no recursion, EPSG:7855 native)
# -----------------------------
def best_property(addr_hint: str | None, latlon: tuple[float, float] | None):
    gdf = props_gdf

    # --- 1. Coordinate-based search (works fine as-is)
    if latlon:
        lat, lon = latlon
        pt = gpd.GeoSeries([Point(lon, lat)], crs="EPSG:4326").to_crs(gdf.crs).iloc[0]
        gdf["centroid_tmp"] = gdf.geometry.centroid
        gdf["dist"] = gdf["centroid_tmp"].distance(pt)
        row = gdf.loc[gdf["dist"].idxmin()]
        cen_wgs = gpd.GeoSeries([row["centroid_tmp"]], crs=gdf.crs).to_crs(4326).iloc[0]
        return {
            "match_type": "coords",
            "Full_Address": row.get("Full_Address"),
            "Suburb": row.get("Suburb"),
            "Postcode": row.get("Postcode"),
            "lat": cen_wgs.y,
            "lon": cen_wgs.x,
            "prop_idx": int(row.name),
        }

    # --- 2. Address-text-based search
    if addr_hint:
        # Normalize hint
        hint = addr_hint.upper().strip()

        # Split into parts (street name, house number, suburb)
        parts = re.split(r"[ ,]+", hint)
        num = None
        for p in parts:
            if re.fullmatch(r"\d+[A-Z]?", p):
                num = p
                break

        # Find suburb match
        suburb_candidates = gdf[gdf["Suburb"].astype(str).str.upper().str.contains("FRANKSTON", na=False)]
        # Use entire GDF if suburb missing
        candidates = suburb_candidates if not suburb_candidates.empty else gdf

        # Street match
        for word in parts:
            matches = candidates[candidates["Street"].astype(str).str.upper().str.contains(word, na=False)]
            if not matches.empty:
                candidates = matches
                break

        # House match
        if num:
            candidates = candidates[candidates["House"].astype(str).str.upper() == num]

        # Return closest centroid match if multiple
        if not candidates.empty:
            row = candidates.iloc[0]
            cen_wgs = gpd.GeoSeries([row.geometry.centroid], crs=gdf.crs).to_crs(4326).iloc[0]
            return {
                "match_type": "address",
                "Full_Address": row.get("Full_Address"),
                "Suburb": row.get("Suburb"),
                "Postcode": row.get("Postcode"),
                "lat": cen_wgs.y,
                "lon": cen_wgs.x,
                "prop_idx": int(row.name),
            }

    return None

def sample_raster_value(raster_path, lon, lat, prefer_band=1, max_depth_m=2.0):
    """
    Sample a flood raster at the given location and scale from 0‚Äì255 to real meters.
    """
    import rasterio
    import numpy as np
    from rasterio.warp import transform

    try:
        with rasterio.open(raster_path) as src:
            xs, ys = transform("EPSG:4326", src.crs, [lon], [lat])

            if not (src.bounds.left <= xs[0] <= src.bounds.right and src.bounds.bottom <= ys[0] <= src.bounds.top):
                return 0.0

            val = list(src.sample([(xs[0], ys[0])], indexes=prefer_band))[0][0]

            # Handle data type scaling
            if val is None or np.isnan(val):
                return 0.0

            # Scale if uint8 (0‚Äì255)
            if src.dtypes[0] == "uint8":
                if val == 0:
                    return 0.0
                else:
                    # Convert 0‚Äì255 ‚Üí 0‚Äìmax_depth_m (typically 2 m)
                    return round((val / 255.0) * max_depth_m, 3)

            # If float rasters (already in meters)
            if np.issubdtype(np.array(val).dtype, np.floating):
                return round(float(val), 3)

            return 0.0
    except Exception as e:
        print(f"‚ö†Ô∏è Raster sampling error for {raster_path}: {e}")
        return 0.0


# def sample_raster_value(lat, lon, raster_path, prefer_band=1):
#     """
#     Sample raster safely at given lat/lon.

#     Rules:
#     - If outside raster -> np.nan
#     - If nodata or >=250 for uint8 -> 0.0 (treat as no flood)
#     - If uint8 and <250 -> value/100.0 (cm -> m)
#     - If negative -> 0.0
#     - Clip to sensible range [0, 10] m
#     """
#     try:
#         with rasterio.open(raster_path) as src:
#             # 1) Transform WGS84 -> raster CRS
#             xs, ys = transform("EPSG:4326", src.crs, [lon], [lat])

#             # 2) Inside bounds?
#             if not (src.bounds.left <= xs[0] <= src.bounds.right and
#                     src.bounds.bottom <= ys[0] <= src.bounds.top):
#                 return np.nan

#             # 3) Sample value from chosen band
#             band = prefer_band if 1 <= prefer_band <= src.count else 1
#             val = list(src.sample([(xs[0], ys[0])], indexes=band))[0][0]

#             # 4) Handle nodata/invalids
#             nodata_val = src.nodata
#             if (val is None) or (nodata_val is not None and val == nodata_val) or np.isnan(val):
#                 return 0.0

#             # 5) Scaling logic
#             if src.dtypes[band-1] == "uint8":
#                 # 255 (and often >=250) used as mask/nodata
#                 if val >= 250:
#                     return 0.0
#                 depth_m = val / 100.0  # cm -> m
#             else:
#                 # Other encodings fallback: if very large, assume mm
#                 depth_m = float(val)
#                 if depth_m > 100:  # defensive
#                     depth_m = depth_m / 1000.0

#             # 6) No negatives; clamp to reasonable range
#             if depth_m < 0:
#                 depth_m = 0.0
#             depth_m = float(np.clip(depth_m, 0.0, 10.0))

#             return round(depth_m, 3)

#     except Exception as e:
#         print(f"‚ö†Ô∏è Raster sampling error ({raster_path}): {e}")
#         return np.nan

def collect_metrics_for_point(lat, lon):
    """
    For each return period (001y..100y) gather dmax/hmax/vmax/z0max
    using first matching raster per metric.
    """
    out = {y: {"dmax": np.nan, "hmax": np.nan, "vmax": np.nan, "z0max": np.nan} for y in YEARS_ORDER}

    for y in YEARS_ORDER:
        subset = raster_catalog[(raster_catalog["return_period"] == y) & (raster_catalog["exists"])]
        if subset.empty:
            continue

        for metric in ["dmax", "hmax", "vmax", "z0max"]:
            # "starts with" is handy if your catalog metric contains suffixes
            cand = subset[subset["metric"].str.startswith(metric)]
            if cand.empty:
                continue
            path = cand.iloc[0]["full_path"]
            # out[y][metric] = sample_raster_value(lat, lon, path, prefer_band=1)
            out[y][metric] = sample_raster_value(path, lon, lat, prefer_band=1)


    return out
def find_nearby_wet_pixel(lat, lon, raster_path, radius_px=4, prefer_band=1):
    """
    Search a small window around the given point for first non-zero
    (useful for seeing if you're just outside the flood extent).
    Returns (value_m, lat, lon) or (None, None, None) if not found.
    """
    try:
        with rasterio.open(raster_path) as src:
            xs, ys = transform("EPSG:4326", src.crs, [lon], [lat])
            row, col = src.index(xs[0], ys[0])

            band = prefer_band if 1 <= prefer_band <= src.count else 1
            window = src.read(band, window=rasterio.windows.Window(col-radius_px, row-radius_px,
                                                                   2*radius_px+1, 2*radius_px+1))
            # scan for first non-zero (and <250 for uint8)
            it = np.nditer(window, flags=['multi_index'])
            while not it.finished:
                v = float(it[0])
                if src.dtypes[band-1] == 'uint8':
                    cond = (0 < v < 250)
                else:
                    cond = v > 0

                if cond:
                    # map back to lat/lon of that pixel
                    rr = row - radius_px + it.multi_index[0]
                    cc = col - radius_px + it.multi_index[1]
                    x, y = src.transform * (cc + 0.5, rr + 0.5)
                    llon, llat = transform(src.crs, "EPSG:4326", [x], [y])
                    # scale as in the sampler
                    depth_m = v/100.0 if src.dtypes[band-1] == 'uint8' else v
                    if depth_m > 100: depth_m /= 1000.0
                    return round(float(depth_m),3), float(llon[0]), float(llat[0])
                it.iternext()
    except Exception as e:
        print("nearby search error:", e)
    return None, None, None


# Non-linear interpolation in log-space of return period (years)
def interp_log_year(metric_by_year: dict[str, float]) -> dict[str, float]:
    # Build arrays of available points
    xs = []
    ys = []
    for ytag, val in metric_by_year.items():
        y = int(ytag.replace("y",""))
        if not np.isnan(val):
            xs.append(y)
            ys.append(val)
    if len(xs) < 2:
        return metric_by_year  # not enough to interpolate
    xs = np.array(xs, dtype=float)
    ys = np.array(ys, dtype=float)
    # interpolate on log(year) ‚Üí log(y+Œµ?) we keep ys linear, xs in log to avoid simple linear
    logx = np.log(xs)
    for ytag in metric_by_year.keys():
        y = int(ytag.replace("y",""))
        if np.isnan(metric_by_year[ytag]):
            metric_by_year[ytag] = float(np.interp(np.log(y), logx, ys))
    return metric_by_year

def summarize_risk(dmax_m: float|None, vmax_ms: float|None) -> str:
    if dmax_m is None or np.isnan(dmax_m): return "Unknown risk ‚Äî no flood depth at this location."
    if dmax_m < 0.05: return "Very low risk ‚Äî nuisance ponding only."
    if dmax_m < 0.3:  return "Low risk ‚Äî shallow overland flow possible."
    if dmax_m < 0.6:  return "Moderate risk ‚Äî curb-depth flow; avoid driving."
    if dmax_m < 1.0:  return "High risk ‚Äî building ingress possible; avoid floodwater."
    return "Extreme risk ‚Äî life-threatening conditions possible."

def safety_tips_vic() -> list[str]:
    return [
        "Never enter floodwater ‚Äî it can be fast-moving or contaminated.",
        "Do not drive through floodwater. As little as 15 cm can float a small car.",
        "Stay informed via VicEmergency app/website and ABC local radio.",
        "For SES flood/storm assistance call 132 500. Call 000 in life-threatening emergencies.",
        "Move vehicles and valuables to higher ground if safe to do so.",
        "Turn off electricity/gas at the mains if flooding is imminent and you can do it safely.",
        "Prepare sandbags to protect low doorways and vents; check council distribution points.",
        "Keep an emergency kit: torch, radio, spare batteries, meds, water, documents.",
        "Avoid walking near drains/culverts; covers may be displaced.",
        "After flooding, assume water is contaminated; photograph damage for insurance."
    ]

# -----------------------------
# UI ‚Äî Input & Clarifications
# -----------------------------
with st.container():
    st.subheader("üí¨ Describe the rainfall/flood observation")
    default_txt = "Severe rainfall of 60 mm in Frankston South for 45 minutes"
    user_txt = st.text_area("Free text", height=90, value=default_txt)

    rain = parse_rain(user_txt)
    dur  = parse_duration(user_txt)
    coords = parse_coords(user_txt)
    addr_hint = parse_address_hint(user_txt)

    missing = clarification_needed(rain, dur, coords, addr_hint)
    if missing:
        st.warning("I need a bit more info to proceed:")
        col1,col2,col3 = st.columns(3)
        with col1:
            rain = st.number_input("Rainfall (mm)", value=rain if rain else 50.0, min_value=0.0, step=1.0)
        with col2:
            dur = st.number_input("Duration (hours)", value=dur if dur else 1.0, min_value=0.0, step=0.25)
        with col3:
            mode = st.radio("Location input", ["Address hint","Coordinates"], index=0 if not coords else 1, horizontal=True)
        if mode=="Coordinates":
            lat = st.number_input("Latitude (e.g., -38.15)", value=coords[0] if coords else -38.1539, step=0.0001, format="%.6f")
            lon = st.number_input("Longitude (e.g., 145.10)", value=coords[1] if coords else 145.1038, step=0.0001, format="%.6f")
            coords = (lat, lon)
            addr_hint = None
        else:
            addr_hint = st.text_input("Address / Street / Suburb", value=addr_hint or "Frankston South")
            coords = None

    st.divider()

# -----------------------------
# Property resolution
# -----------------------------
prop = best_property(addr_hint, coords)
if not prop:
    st.error("I couldn't resolve a property from that input. Please provide a clearer address or coordinates.")
    st.stop()
prop_point = Point(prop["lon"], prop["lat"])
sub_match = subcatchments_gdf[subcatchments_gdf.contains(prop_point)]
if not sub_match.empty:
    sub_name = sub_match.iloc[0].get("Subcatchment", "Unknown")
    st.caption(f"üìç Subcatchment: {sub_name}")
else:
    st.caption("üìç Outside known subcatchments.")



st.success(f"üìç Matched property: **{prop['Full_Address']}**")
st.caption(f"Lat/Lon: {prop['lat']:.6f}, {prop['lon']:.6f} | Suburb: {prop['Suburb']} {prop['Postcode']} | via {prop['match_type']}")

# -----------------------------
# Flood metrics for this point
# -----------------------------
st.subheader("üìà Flood metrics by return period (at property location)")
raw = collect_metrics_for_point(prop["lat"], prop["lon"])

# Interpolate non-linearly (log years) for each metric
interp = {}
for mkey in ["dmax","hmax","vmax","z0max"]:
    series = {y: raw[y][mkey] for y in YEARS_ORDER}
    interp[mkey] = interp_log_year(series)

df = pd.DataFrame({
    "ReturnPeriod": YEARS_ORDER,
    "Depth_dmax_m": [interp["dmax"][y] for y in YEARS_ORDER],
    "WaterLevel_hmax_m": [interp["hmax"][y] for y in YEARS_ORDER],
    "Velocity_vmax_ms": [interp["vmax"][y] for y in YEARS_ORDER],
    "HazardIndex_z0": [interp["z0max"][y] for y in YEARS_ORDER],
})
st.dataframe(df, use_container_width=True)

# Risk summary chosen at nearest ‚Äúmatching‚Äù scenario (by rainfall)
def choose_scenario_from_rain(r):
    if r is None: return "001y"
    if r < 20: return "005y"
    if r < 40: return "010y"
    if r < 60: return "020y"
    if r < 80: return "050y"
    return "100y"

# New IFD-based rainfall mapping
if not np.isnan(rain) and not np.isnan(dur):
    scen_rp = estimate_return_period_from_ifd(rain, dur, prop["lat"], prop["lon"], ifd_table)

    # Handle missing or invalid return period safely
    if scen_rp is None or np.isnan(scen_rp):
        scen = "010y"
        st.caption("üåÄ Unable to estimate return period ‚Äî defaulting to 10-year event (010y).")
    else:
        scen_rounded = int(round(scen_rp / 5) * 5)
        scen = f"{scen_rounded:03d}y"
        st.caption(f"üåÄ Estimated event intensity corresponds to ~{scen_rp:.1f}-year storm ({scen})")

else:
    scen = "010y"
    st.caption("üåÄ Missing rainfall or duration ‚Äî defaulting to 10-year event (010y).")


d_here = interp["dmax"].get(scen)
v_here = interp["vmax"].get(scen)
st.info(f"Scenario selected from rainfall: **{scen}** ‚Üí Depth ‚âà **{d_here if not np.isnan(d_here) else 'N/A'} m**, Velocity ‚âà **{v_here if not np.isnan(v_here) else 'N/A'} m/s**")
st.warning(summarize_risk(d_here, v_here))

# -----------------------------
# Map with hazard zones
# -----------------------------
st.subheader("üó∫Ô∏è Map & Safety Zones")
m = folium.Map(location=[prop["lat"], prop["lon"]], zoom_start=15)
popup = f"{prop['Full_Address']}<br><b>{scen}</b>: d={d_here if not np.isnan(d_here) else 'N/A'} m"
folium.Marker([prop["lat"], prop["lon"]], popup=popup, icon=folium.Icon(color="red")).add_to(m)
# zones: 100m, 250m, 500m (buffer in meters requires projected CRS ‚Üí approximate with folium radius)
for radius, col in [(100, "#FF0000"), (250, "#FFA500"), (500, "#3388ff")]:
    folium.Circle(
        location=[prop["lat"], prop["lon"]],
        radius=radius,
        color=col, fill=True, fill_opacity=0.08, weight=1,
        tooltip=f"Zone {radius} m"
    ).add_to(m)
    
st_folium(m, height=520, use_container_width=True)



# -----------------------------
# Sentiment + Safety message
# -----------------------------
st.subheader("üß† Advisory")
sev = "severe" if (rain and rain>=60) or (d_here and not np.isnan(d_here) and d_here>=0.6) else "moderate" if (rain and rain>=20) else "low"
st.write(f"**Overall sentiment:** {('üö® High Concern' if sev=='severe' else '‚ö†Ô∏è Elevated Caution' if sev=='moderate' else '‚úÖ Low Concern')}")

with st.expander("Safety guidance (Victoria / SES-aligned)"):
    for tip in safety_tips_vic():
        st.markdown(f"- {tip}")

# -----------------------------
# Final summary
# -----------------------------
st.divider()
st.markdown("### üìã Summary")
st.markdown(f"""
- **Rainfall reported:** {rain if rain is not None else 'Not provided'} mm  
- **Duration:** {dur if dur is not None else 'Not provided'} h  
- **Matched property:** {prop['Full_Address']}  
- **Chosen scenario:** {scen}  
- **Estimated depth:** {d_here if d_here is not None and not np.isnan(d_here) else 'No data'} m  
- **Estimated velocity:** {v_here if v_here is not None and not np.isnan(v_here) else 'No data'} m/s  
- **Risk level:** {('Extreme/High' if sev=='severe' else 'Moderate' if sev=='moderate' else 'Low')}  
- **Timestamp:** {datetime.datetime.now().isoformat(timespec='seconds')}
""")

st.download_button(
    "‚¨áÔ∏è Download Flood Report (CSV)",
    df.to_csv(index=False).encode('utf-8'),
    "flood_metrics.csv",
    "text/csv"
)

